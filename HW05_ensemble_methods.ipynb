{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "HW05 ensemble_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wksmirnowa/machinelearning_homeworks/blob/master/HW05_ensemble_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_j3U7Qu8OG-",
        "colab_type": "text"
      },
      "source": [
        "# Ансамбли\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kamHTSc5xlqX",
        "colab_type": "text"
      },
      "source": [
        "## Импорты и важные функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hutXdd3J8OHF",
        "colab_type": "code",
        "outputId": "c0f7e75b-0b40-4627-c3ef-30d831ea9320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mlxtend.plotting import plot_learning_curves\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from string import punctuation\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_extraction.text import *\n",
        "from sklearn.metrics import *\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression, LinearRegression, LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier,  BaggingClassifier, BaggingRegressor, RandomTreesEmbedding, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    return words\n",
        "\n",
        "def normalize_en(text):\n",
        "\n",
        "  words = []\n",
        "  tokenized_text = tokenize(text)\n",
        "  for word, tag in pos_tag(tokenized_text):\n",
        "    lemtag = tag[0].lower()\n",
        "    lemtag = lemtag if lemtag in ['a', 'r', 'n', 'v'] else None\n",
        "\n",
        "    if not lemtag:\n",
        "      lemma = word\n",
        "      words.append(lemma)\n",
        "    else:\n",
        "      lemma = lem.lemmatize(word, lemtag)\n",
        "      words.append(lemma)\n",
        "\n",
        "  return \" \".join(words)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yly4nQ62oUF8",
        "colab_type": "code",
        "outputId": "4db7f2c7-7771-45f8-a8f8-757c5d1c47f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/TatianaShavrina/hse_ml_m1/master/ensembles/complaints.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-24 14:17:52--  https://raw.githubusercontent.com/TatianaShavrina/hse_ml_m1/master/ensembles/complaints.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4148456 (4.0M) [text/plain]\n",
            "Saving to: ‘complaints.csv’\n",
            "\n",
            "\rcomplaints.csv        0%[                    ]       0  --.-KB/s               \rcomplaints.csv      100%[===================>]   3.96M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-24 14:17:52 (29.8 MB/s) - ‘complaints.csv’ saved [4148456/4148456]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMCYR16xi7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_metrics(y_test, predictions):\n",
        "  print(\"Precision: {0:6.2f}\".format(precision_score(y_test, predictions, average='macro')))\n",
        "  print(\"Recall: {0:6.2f}\".format(recall_score(y_test, predictions, average='macro')))\n",
        "  print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, predictions, average='macro')))\n",
        "  print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, predictions)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg5ye6Vsxrkb",
        "colab_type": "text"
      },
      "source": [
        "## Работа с данными"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gJ3DyKPoebU",
        "colab_type": "code",
        "outputId": "8a4c7d7e-6e92-44dc-9ccb-2ee2ca2e42c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data = pd.read_csv('complaints.csv', sep='\\t')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COMPLAINT_ID</th>\n",
              "      <th>DATE</th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>ISSUE_ID</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3178905</td>\n",
              "      <td>03/13/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>318</td>\n",
              "      <td>go year . contact advis never took loan . advi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3175952</td>\n",
              "      <td>03/12/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>349</td>\n",
              "      <td>mail valid debt xx/xx/19 valid receiv , receiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3174747</td>\n",
              "      <td>03/09/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>16</td>\n",
              "      <td>xx/xx/xxxx appli receiv onlin loan bluechip fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3173291</td>\n",
              "      <td>03/08/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>16</td>\n",
              "      <td>xx/xx/xxxx appli receiv onlin loan . loan amou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3172221</td>\n",
              "      <td>03/07/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>48</td>\n",
              "      <td>told husband left bill . debt would pay within...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   COMPLAINT_ID  ...                                       cleaned_text\n",
              "0       3178905  ...  go year . contact advis never took loan . advi...\n",
              "1       3175952  ...  mail valid debt xx/xx/19 valid receiv , receiv...\n",
              "2       3174747  ...  xx/xx/xxxx appli receiv onlin loan bluechip fi...\n",
              "3       3173291  ...  xx/xx/xxxx appli receiv onlin loan . loan amou...\n",
              "4       3172221  ...  told husband left bill . debt would pay within...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5_D2BAxTEW",
        "colab_type": "text"
      },
      "source": [
        "Предобработаем тексты с помощью nltk. В данных много несловарных форм, но попадаются и обычные словарные слова, с ними лемматизация должна сработать и тем самым немного повысить метрики"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgJgXFyItJfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_texts = []\n",
        "for line in data.cleaned_text:\n",
        "  norm_texts.append(normalize_en(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DhqRHc1uXEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['norm_text'] = norm_texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S5zF8-jufhR",
        "colab_type": "code",
        "outputId": "03d4735c-ffdb-4860-d900-3c3586096c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COMPLAINT_ID</th>\n",
              "      <th>DATE</th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>ISSUE_ID</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>norm_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3178905</td>\n",
              "      <td>03/13/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>318</td>\n",
              "      <td>go year . contact advis never took loan . advi...</td>\n",
              "      <td>go year contact advis never take loan advis se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3175952</td>\n",
              "      <td>03/12/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>349</td>\n",
              "      <td>mail valid debt xx/xx/19 valid receiv , receiv...</td>\n",
              "      <td>mail valid debt xx/xx/19 valid receiv receiv c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3174747</td>\n",
              "      <td>03/09/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>16</td>\n",
              "      <td>xx/xx/xxxx appli receiv onlin loan bluechip fi...</td>\n",
              "      <td>xx/xx/xxxx appli receiv onlin loan bluechip fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3173291</td>\n",
              "      <td>03/08/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>16</td>\n",
              "      <td>xx/xx/xxxx appli receiv onlin loan . loan amou...</td>\n",
              "      <td>xx/xx/xxxx appli receiv onlin loan loan amount...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3172221</td>\n",
              "      <td>03/07/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>48</td>\n",
              "      <td>told husband left bill . debt would pay within...</td>\n",
              "      <td>told husband leave bill debt would pay within ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3172723</td>\n",
              "      <td>03/07/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>310</td>\n",
              "      <td>payday loan , becam abl pay . everyon famili c...</td>\n",
              "      <td>payday loan becam abl pay everyon famili conta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3171497</td>\n",
              "      <td>03/06/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>320</td>\n",
              "      <td>within past week , contact oracl financi group...</td>\n",
              "      <td>within past week contact oracl financi group l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3171674</td>\n",
              "      <td>03/06/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>103</td>\n",
              "      <td>call work said courthous owe money . got phone...</td>\n",
              "      <td>call work say courthous owe money get phone wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3170784</td>\n",
              "      <td>03/06/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>318</td>\n",
              "      <td>today xx/xx/xxxx cst contact ks . receiv appro...</td>\n",
              "      <td>today xx/xx/xxxx cst contact k receiv approxim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3171479</td>\n",
              "      <td>03/06/2019</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>cashcal attempt collect fraudul debt . ident s...</td>\n",
              "      <td>cashcal attempt collect fraudul debt ident ste...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   COMPLAINT_ID  ...                                          norm_text\n",
              "0       3178905  ...  go year contact advis never take loan advis se...\n",
              "1       3175952  ...  mail valid debt xx/xx/19 valid receiv receiv c...\n",
              "2       3174747  ...  xx/xx/xxxx appli receiv onlin loan bluechip fi...\n",
              "3       3173291  ...  xx/xx/xxxx appli receiv onlin loan loan amount...\n",
              "4       3172221  ...  told husband leave bill debt would pay within ...\n",
              "5       3172723  ...  payday loan becam abl pay everyon famili conta...\n",
              "6       3171497  ...  within past week contact oracl financi group l...\n",
              "7       3171674  ...  call work say courthous owe money get phone wo...\n",
              "8       3170784  ...  today xx/xx/xxxx cst contact k receiv approxim...\n",
              "9       3171479  ...  cashcal attempt collect fraudul debt ident ste...\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzXmLMfU8OHg",
        "colab_type": "text"
      },
      "source": [
        "## Задание 1. Voting Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G1FoOg1-aPm",
        "colab_type": "text"
      },
      "source": [
        "Задание: попробуйте поднять качество классификации до 70% с помощью известных вам методов препроцессинга и Voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQipLvKDCc4q",
        "colab_type": "text"
      },
      "source": [
        "### Способ 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47HyIqi8OHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data[\"PRODUCT_ID\"]\n",
        "X = data[\"norm_text\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) #test_size=0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIQK1n428OIH",
        "colab_type": "code",
        "outputId": "277ecca3-70b2-48ad-fcd5-67cd27543ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "clf1 = LogisticRegression(penalty='l2', C=0.9, multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "clf2 = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=20, min_samples_leaf=5, max_features=300, random_state=42, n_jobs=-1) #class_weight='balanced'\n",
        "clf3 = GaussianNB(var_smoothing=1e-90)\n",
        "\n",
        "eclf = VotingClassifier(estimators=[\n",
        "        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard', n_jobs=10)\n",
        "\n",
        "voting = Pipeline([\n",
        "    ('vect', CountVectorizer(min_df=2, analyzer='word', max_features=500, ngram_range=(1,3), stop_words='english')), #ngram_range=(1,2), max_df=300, \n",
        "    ('tfidf', TfidfTransformer(norm='l2', sublinear_tf=True)),\n",
        "    ('to_dense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)), \n",
        "    ('clf', eclf),\n",
        "    ])\n",
        "voting = voting.fit(X_train, y_train)\n",
        "predictions = voting.predict(X_test)\n",
        "show_metrics(y_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:   0.70\n",
            "Recall:   0.65\n",
            "F1-measure:   0.65\n",
            "Accuracy:   0.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RFrlNr_CfcV",
        "colab_type": "text"
      },
      "source": [
        "### Способ 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8LJ7FcRCXrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(X_train, size=100)\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.dim = 100\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in word_tokenize(words) if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1aQhg2W-5ju",
        "colab_type": "code",
        "outputId": "2470f9c7-07bf-4497-bbd9-c2ba534d79ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "clf = Pipeline([\n",
        "    ('feats', FeatureUnion([\n",
        "        ('tfidf', TfidfVectorizer(min_df=2, analyzer='word', max_features=500, ngram_range=(1,3), stop_words='english')),\n",
        "        (\"word2vec vectorizer\", MeanEmbeddingVectorizer(model))])),\n",
        "    ('clf', ExtraTreesClassifier(n_estimators=200))\n",
        "    ])\n",
        "\n",
        "clf = clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "show_metrics(y_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:   0.71\n",
            "Recall:   0.70\n",
            "F1-measure:   0.70\n",
            "Accuracy:   0.70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phTOhPkPZYfg",
        "colab_type": "text"
      },
      "source": [
        "## Задание 2.\n",
        "\n",
        "Основываясь на своих знаниях об ансамблировании различных моделей, на данных из complaints.csv достигните наилучшего результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw8H_0cx8OK0",
        "colab_type": "text"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geHFQvfYaIPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMTh4zeSaxLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(min_df=2, analyzer='word', max_features=500, ngram_range=(1,3), stop_words='english')\n",
        "y = data[\"PRODUCT_ID\"]\n",
        "X = cv.fit_transform(data[\"norm_text\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJJODyJ-8OLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf1 = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=5, min_samples_split=3, min_samples_leaf=2, max_features=100, random_state=42)\n",
        "clf2 = SVC(C=5.0, kernel='poly', degree=5, gamma='auto', coef0=1.0, max_iter=1000, random_state=42)\n",
        "clf3 = MultinomialNB(alpha=0.5)    \n",
        "clf4 = LogisticRegression(penalty='l2', C=0.9, multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "\n",
        "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=200, max_samples=0.9, max_features=0.9, n_jobs=5)\n",
        "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=200, max_samples=0.9, max_features=0.9, n_jobs=5)\n",
        "bagging3 = BaggingClassifier(base_estimator=clf3, n_estimators=200, max_samples=0.9, max_features=0.9, n_jobs=5)\n",
        "bagging4 = BaggingClassifier(base_estimator=clf4, n_estimators=200, max_samples=0.9, max_features=0.9, n_jobs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br1KH2uPJG_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(n_splits=3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrMxZNYGcbiF",
        "colab_type": "code",
        "outputId": "4b2749f1-b9ce-49ed-d4f3-937032ec7c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "clf_list = [bagging1, bagging2, bagging3, bagging4]\n",
        "for clf in clf_list:        \n",
        "    scores = cross_val_score(clf, X, y, cv=kfold, scoring='accuracy')\n",
        "    print(f'{clf}')\n",
        "    print (\"Accuracy: %.2f (+/- %.2f)\" %(scores.mean(), scores.std()))\n",
        "        \n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    show_metrics(y_test, predictions)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
            "                                                        class_weight=None,\n",
            "                                                        criterion='entropy',\n",
            "                                                        max_depth=5,\n",
            "                                                        max_features=100,\n",
            "                                                        max_leaf_nodes=None,\n",
            "                                                        min_impurity_decrease=0.0,\n",
            "                                                        min_impurity_split=None,\n",
            "                                                        min_samples_leaf=2,\n",
            "                                                        min_samples_split=3,\n",
            "                                                        min_weight_fraction_leaf=0.0,\n",
            "                                                        presort='deprecated',\n",
            "                                                        random_state=42,\n",
            "                                                        splitter='random'),\n",
            "                  bootstrap=True, bootstrap_features=False, max_features=0.9,\n",
            "                  max_samples=0.9, n_estimators=200, n_jobs=5, oob_score=False,\n",
            "                  random_state=None, verbose=0, warm_start=False)\n",
            "Accuracy: 0.20 (+/- 0.15)\n",
            "Precision:   0.68\n",
            "Recall:   0.61\n",
            "F1-measure:   0.60\n",
            "Accuracy:   0.61\n",
            "BaggingClassifier(base_estimator=SVC(C=5.0, break_ties=False, cache_size=200,\n",
            "                                     class_weight=None, coef0=1.0,\n",
            "                                     decision_function_shape='ovr', degree=5,\n",
            "                                     gamma='auto', kernel='poly', max_iter=1000,\n",
            "                                     probability=False, random_state=42,\n",
            "                                     shrinking=True, tol=0.001, verbose=False),\n",
            "                  bootstrap=True, bootstrap_features=False, max_features=0.9,\n",
            "                  max_samples=0.9, n_estimators=200, n_jobs=5, oob_score=False,\n",
            "                  random_state=None, verbose=0, warm_start=False)\n",
            "Accuracy: 0.39 (+/- 0.21)\n",
            "Precision:   0.67\n",
            "Recall:   0.66\n",
            "F1-measure:   0.66\n",
            "Accuracy:   0.66\n",
            "BaggingClassifier(base_estimator=MultinomialNB(alpha=0.5, class_prior=None,\n",
            "                                               fit_prior=True),\n",
            "                  bootstrap=True, bootstrap_features=False, max_features=0.9,\n",
            "                  max_samples=0.9, n_estimators=200, n_jobs=5, oob_score=False,\n",
            "                  random_state=None, verbose=0, warm_start=False)\n",
            "Accuracy: 0.53 (+/- 0.20)\n",
            "Precision:   0.66\n",
            "Recall:   0.65\n",
            "F1-measure:   0.65\n",
            "Accuracy:   0.65\n",
            "BaggingClassifier(base_estimator=LogisticRegression(C=0.9, class_weight=None,\n",
            "                                                    dual=False,\n",
            "                                                    fit_intercept=True,\n",
            "                                                    intercept_scaling=1,\n",
            "                                                    l1_ratio=None,\n",
            "                                                    max_iter=1000,\n",
            "                                                    multi_class='multinomial',\n",
            "                                                    n_jobs=None, penalty='l2',\n",
            "                                                    random_state=42,\n",
            "                                                    solver='lbfgs', tol=0.0001,\n",
            "                                                    verbose=0,\n",
            "                                                    warm_start=False),\n",
            "                  bootstrap=True, bootstrap_features=False, max_features=0.9,\n",
            "                  max_samples=0.9, n_estimators=200, n_jobs=5, oob_score=False,\n",
            "                  random_state=None, verbose=0, warm_start=False)\n",
            "Accuracy: 0.40 (+/- 0.22)\n",
            "Precision:   0.65\n",
            "Recall:   0.65\n",
            "F1-measure:   0.65\n",
            "Accuracy:   0.64\n",
            "CPU times: user 5.5 s, sys: 1.22 s, total: 6.72 s\n",
            "Wall time: 51min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNdtnRt_C1Vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "bc81d78c-7a1a-4a7c-a364-8806c370442a"
      },
      "source": [
        "clf_rf = RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_leaf=5,\n",
        "                             class_weight='balanced', random_state=42)\n",
        "\n",
        "\n",
        "scores = cross_val_score(clf_rf, X, y, cv=kfold, scoring='accuracy')\n",
        "print(f'{clf_rf}')\n",
        "print (\"Accuracy: %.2f (+/- %.2f)\" %(scores.mean(), scores.std()))\n",
        "        \n",
        "clf_rf.fit(X_train, y_train)\n",
        "predictions = clf_rf.predict(X_test)\n",
        "show_metrics(y_test, predictions)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
            "                       criterion='gini', max_depth=20, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=5, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
            "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
            "                       warm_start=False)\n",
            "Accuracy: 0.49 (+/- 0.22)\n",
            "Precision:   0.68\n",
            "Recall:   0.67\n",
            "F1-measure:   0.67\n",
            "Accuracy:   0.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSlXgPlXH1o2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "34d63e48-9f48-49b9-b1b0-0881ae4473da"
      },
      "source": [
        "clf_et = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=20, min_samples_leaf=5, max_features=300, bootstrap=True,\n",
        "                             class_weight='balanced', random_state=0, max_samples=300)\n",
        "\n",
        "\n",
        "scores = cross_val_score(clf_et, X, y, cv=kfold, scoring='accuracy')\n",
        "print(f'{clf_et}')\n",
        "print (\"Accuracy: %.2f (+/- %.2f)\" %(scores.mean(), scores.std()))\n",
        "        \n",
        "clf_et.fit(X_train, y_train)\n",
        "predictions = clf_et.predict(X_test)\n",
        "show_metrics(y_test, predictions)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
            "                     criterion='gini', max_depth=20, max_features=300,\n",
            "                     max_leaf_nodes=None, max_samples=300,\n",
            "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                     min_samples_leaf=5, min_samples_split=2,\n",
            "                     min_weight_fraction_leaf=0.0, n_estimators=200,\n",
            "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
            "                     warm_start=False)\n",
            "Accuracy: 0.36 (+/- 0.24)\n",
            "Precision:   0.64\n",
            "Recall:   0.62\n",
            "F1-measure:   0.62\n",
            "Accuracy:   0.62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IqkbSw8OMW",
        "colab_type": "text"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLWqzmZyMZYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(min_df=2, analyzer='word', max_features=500, ngram_range=(1,3), stop_words='english')\n",
        "y = data[\"PRODUCT_ID\"]\n",
        "X = cv.fit_transform(data[\"norm_text\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT7Yfl5nMhT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf1 = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=5, min_samples_split=3, min_samples_leaf=2, max_features=100, random_state=42)\n",
        "clf2 = LogisticRegression(penalty='l2', C=0.9, multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "clf3 = MultinomialNB(alpha=0.5)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PuAbDzEN_Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(n_splits=3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrrPeQwMlZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8d5fbd1-e296-4680-a567-479668f5a655"
      },
      "source": [
        "%%time\n",
        "num_est = [1, 2, 3, 5]\n",
        "clf_list = [clf1, clf2, clf3]\n",
        "labels = ['AdaBoost (n_est=10)', 'AdaBoost (n_est=50)', 'AdaBoost (n_est=100)', 'AdaBoost (n_est=200)']\n",
        "for n_est in num_est:\n",
        "  for clf in clf_list:\n",
        "    print(f'{n_est}, {clf}')\n",
        "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)  \n",
        "    boosting.fit(X, y)\n",
        "    predictions = clf_et.predict(X)\n",
        "    show_metrics(y, predictions)\n",
        "    scores = cross_val_score(boosting, X, y, cv=kfold, scoring='accuracy')\n",
        "    print (\"Accuracy: %.2f (+/- %.2f)\" %(scores.mean(), scores.std()))\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1, DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
            "                       max_depth=5, max_features=100, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=2, min_samples_split=3,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=42, splitter='random')\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.10 (+/- 0.10)\n",
            "1, LogisticRegression(C=0.9, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.12 (+/- 0.11)\n",
            "1, MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.07 (+/- 0.09)\n",
            "2, DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
            "                       max_depth=5, max_features=100, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=2, min_samples_split=3,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=42, splitter='random')\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.15 (+/- 0.10)\n",
            "2, LogisticRegression(C=0.9, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.18 (+/- 0.16)\n",
            "2, MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.50 (+/- 0.13)\n",
            "3, DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
            "                       max_depth=5, max_features=100, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=2, min_samples_split=3,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=42, splitter='random')\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.21 (+/- 0.15)\n",
            "3, LogisticRegression(C=0.9, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.21 (+/- 0.17)\n",
            "3, MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.40 (+/- 0.23)\n",
            "5, DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
            "                       max_depth=5, max_features=100, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=2, min_samples_split=3,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=42, splitter='random')\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.24 (+/- 0.15)\n",
            "5, LogisticRegression(C=0.9, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
            "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.25 (+/- 0.19)\n",
            "5, MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
            "Precision:   0.66\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.19 (+/- 0.11)\n",
            "CPU times: user 9.95 s, sys: 40 ms, total: 9.99 s\n",
            "Wall time: 9.99 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jU7yzBc8ON2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AouToHni8ON-",
        "colab_type": "code",
        "outputId": "1654d3c8-00c2-4921-ddef-4f392feb1fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "clf = xgb.XGBClassifier(objective='multi:softmax')\n",
        "clf.fit(X_train,  y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "show_metrics(y_test, predictions)\n",
        "scores = cross_val_score(sclf, X, y, cv=kfold, scoring='accuracy')\n",
        "print (\"Accuracy: %.2f (+/- %.2f)\" %(scores.mean(), scores.std()))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:   0.65\n",
            "Recall:   0.64\n",
            "F1-measure:   0.64\n",
            "Accuracy:   0.64\n",
            "Accuracy: 0.23 (+/- 0.17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os4OarLQ8OOJ",
        "colab_type": "text"
      },
      "source": [
        "### Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNgtGyN_PtiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(min_df=2, analyzer='word', max_features=500, ngram_range=(1,3), stop_words='english')\n",
        "y = data[\"PRODUCT_ID\"]\n",
        "X = cv.fit_transform(data[\"norm_text\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OHYeJrFPIUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf1 = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=5, min_samples_split=3, min_samples_leaf=2, max_features=100, random_state=42)\n",
        "clf2 = SVC(C=5.0, kernel='poly', degree=5, gamma='auto', coef0=1.0, max_iter=1000, random_state=42)\n",
        "clf3 = MultinomialNB(alpha=0.5) \n",
        "clf4 = LogisticRegression(penalty='l2', C=0.9, multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
        "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3, clf4], \n",
        "                          meta_classifier=clf4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpgOYGIRZ0hw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f38a23a2-0681-49cd-c680-6a094f9b8ea6"
      },
      "source": [
        "sclf.fit(X, y)\n",
        "predictions = sclf.predict(X)\n",
        "show_metrics(y, predictions)\n",
        "scores = cross_val_score(sclf, X, y, cv=kfold, scoring='accuracy')\n",
        "print (\"Accuracy: %.2f (+/- %.2f)\" %(scores.mean(), scores.std()))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:   0.59\n",
            "Recall:   0.59\n",
            "F1-measure:   0.54\n",
            "Accuracy:   0.60\n",
            "Accuracy: 0.23 (+/- 0.17)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}