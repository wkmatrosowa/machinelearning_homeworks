{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Классификация текстов\n",
    "\n",
    "При подготовке использовались материалы Екатерины Артемовой\n",
    "( echernyak@hse.ru )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Постановка задачи\n",
    "\n",
    "* $d \\in D$ – документы\n",
    "* $c \\in C$ – классы \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Бинарная классификация: $C = \\{0, 1\\}$ \n",
    "* Многоклассовая классификация [multiclass classification]: $C = \\{0, ..., K\\}$\n",
    "* Многотемная классификация [multi-label classification]: $C = \\{0,1\\}^K$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Примеры\n",
    "\n",
    "* Фильтрация спама: $C = \\{spam, ham\\}$ – бинарная классификация\n",
    "* Классификация по тональности: $C =  \\{neutral, positive, negative\\}$ – классификация с тремя классами\n",
    "* Рубрикация: $C \\in \\{религия, праздники, спорт, фестивали, ... \\}$ – классификация на несколько тем\n",
    "* Определение языка текста: $C \\in \\{Rus, Sr, Ukr, Bel, Kaz, Bul ... \\}$\n",
    "* Определение авторства:\n",
    "    * Этим ли автором написан текст: $ C = \\{0, 1\\}$?\n",
    "    * Кем из этих авторов написан текст: $ C = \\{a_1, a_2, a_3, ... \\}$?\n",
    "    * Пол автора: $ C = \\{f, m\\}$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Методы классификации\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### По правилам\n",
    "\n",
    "* Если в предложении встречается личное местоимение первого лица и глагол с окончанием женского рода, то пол автора = $f$.\n",
    "* Если доля положительно окрашенных прилагательтельных в отзыве больше доли отрицательно окрашенных прилагательных, то отзыв относится к классу $posititive$.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### С использованием алгоритмов машинного обучения \n",
    "\n",
    "$ \\gamma : D \\rightarrow C$ - алгоритм классификации\n",
    "\n",
    "$({D^{train}, C^{train}})$ – обучающее множество \n",
    "\n",
    "$({D^{test}, C^{test}})$ – тестовое множество \n",
    "\n",
    "#### Основные методы\n",
    "* Метод наивного Байеса\n",
    "* Логистическая регрессия \n",
    "* Сверточные нейронные сети\n",
    "* FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Меры качества бинарной классификации \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" rowspan=\"2\"></th>\n",
    "    <th colspan=\"2\">gold <br>standart</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>positive</td>\n",
    "    <td>negative</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">classification <br>output</td>\n",
    "    <td>positive</td>\n",
    "    <td>$tp$</td>\n",
    "    <td>$fp$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>negative</td>\n",
    "    <td>$fn$</td>\n",
    "    <td>$tn$</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "tp – true positive (сработало верно, это правда положительное)\n",
    "fn – false negative (сработало неверно, это не негативное)\n",
    "\n",
    "Точность – это тру позитивы разделить на всю 1 строку (тру и фолс)\n",
    "\n",
    "Полнота – сколько алгоритм нашел реальных позитивных разделить на то, что реально\n",
    "позитивное (тру позитив + ложный негатив)\n",
    "\n",
    "Таким образом, аккуратность покажет, сколько всего верных значений было найдено\n",
    "по сравнению со всеми остальными значениями"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$precision = Pr =  \\frac{tp}{tp+fp} $ – точность \n",
    "\n",
    "$recall = R = \\frac{tp}{tp+fn} $ – полнота \n",
    "\n",
    "$F_2 = \\frac{2 Pr * R}{Pr + R}$ – $F$-мера \n",
    "\n",
    "$accuracy = \\frac{tp + tn}{tp + fp + fn + tn}$ –  аккуратность  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*ROC кривая* (receiver operating characteristic) - отображает отношение между чувствительностью алгоритма (*true positive rate*) и специфичностью алгоритма (*false positive rate*)\n",
    "\n",
    "*true positive rate* $= \\frac{tp}{tp+fn}$          \n",
    "\n",
    "*false positive rate* $=  1 - \\frac{tn}{fp+tn}$     \n",
    "\n",
    "\n",
    "*ROC AUC* - area under the curve, площадь под ROC-кривой  - количественная интерпретация работы алгоритма при различных порогах интерпретации\n",
    "\n",
    "Если площадь под roc-кривой 0,5, алгоритм никуда не годится.\n",
    "Если площадь меньше, то можно моменять местами X и Y оси (классы)\n",
    "Если площадь больше, уже лучше.\n",
    "Больше 1 быть не может."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y_true = np.array([0, 1, 1, 1, 0]) #как было в корпусе\n",
    "y_predicted = np.array([0.1, 0.7, 0.4, 0.2, 0.27]) #как сработал алгоритм\n",
    "fpr, tpr, _ = roc_curve(y_true, y_predicted)\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Test')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "#tp 1/3, fp 0, tn 2/2=1, fn 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_true, y_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Меры качества многоклассовой классификации \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th></th>\n",
    "    <th colspan=\"3\">gold <br>standart</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "    <td>$class_1$</td>\n",
    "    <td>$class_2$</td>\n",
    "    <td>$class_3$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">classification <br>output</td>\n",
    "    <td>$class_1$</td>\n",
    "    <td>$tp_1$</td>\n",
    "    <td>$fp_{12}$</td>\n",
    "    <td>$fp_{13}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$class_2$</td>\n",
    "    <td>$fn_{21}$</td>\n",
    "    <td>$tp_2$</td>\n",
    "    <td>$fp_{23}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$class_3$</td>\n",
    "    <td>$fn_{31}$</td>\n",
    "    <td>$fn_{32}$</td>\n",
    "    <td>$tp_3$</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "В 1 строке – все случаи, когда мы назвали классом 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Микро-усреднение:\n",
    "\n",
    "$micro-precision = $micro-Pr$ =  \\frac{\\sum tp_i}{\\sum tp_i + \\sum fp_i} $ \n",
    "\n",
    "$micro-recall = $micro-R$ = \\frac{\\sum tp_i}{\\sum tp_i+ \\sum fn_i } $\n",
    "\n",
    "Макро-усреднение:\n",
    "\n",
    "$macro-precision = $macro-Pr$ =  \\frac{\\sum Pr_i}{|C|} $\n",
    "\n",
    "$macro-recall = $macro-R$ = \\frac{\\sum R_i}{|C|} $ \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Источники данных\n",
    "* IMDB \n",
    "* newsgroups 20\n",
    "* Reuters \n",
    "* Кинопоиск\n",
    "* Wiki\n",
    "* \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Напоминание) Векторная модель: документ – вектор признаков \n",
    "\n",
    "* $d \\in D$ – документы\n",
    "* $w \\in V$ – словарь, всего слов |V|\n",
    "\n",
    "* традиционное представление: одно слово – одна размерность в векторной модели: $\\vec{d_i} = <f_1, ... , f_{|V|}> $\n",
    "* $f$ – компоненты вектора – могут быть:\n",
    "    * 0 и 1\n",
    "    * частотами\n",
    "    * $tf-idf$ весами\n",
    "* с использованием распределенных представлений слов [word embeddings]:\n",
    "    * покомпонентное среднее векторов слов, входящих в текст\n",
    "    * покомпонентный максимум векторов слов, входящих в текст\n",
    "* с использованием распределенных представлений текстов [doc embeddings]:\n",
    "    * doc2vec\n",
    "    * fastText\n",
    "    * снижение размерности в векторной модели, в т. ч. сингулярное разложение [singular value decomposition, SVD]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Напоминание) Вычисление расстояния / близости между документами \n",
    "\n",
    "Евклидово расстояние: $ dist( \\vec{d_i}, \\vec{d_j}) = \\sqrt { \\sum_{k} ( d_i^k - d_j^k)^2 }$\n",
    "\n",
    "Косинусная мера близости: $ sim( \\vec{d_i}, \\vec{d_j}) =  \\cos(\\theta )=  \\frac{ \\vec{d_i}\\cdot \\vec{d_j} }{\\| \\vec{d_i} \\|_{2}\\|\\vec{d_j} \\|_{2}}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Метод ближайшего соседа  [Nearest neighbor classifier]\n",
    "\n",
    "Найдем $k$ ближайших соседей (самых близких документов) для документа $d$. Посмотрим на то, каким классам относятся документы: выберем модальный класс – будем считать его классом $d$.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(twenty_train.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "twenty_train.data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data) #Learn the vocabulary dictionary and return term-document matrix.\n",
    "X_train_counts.shape\n",
    "#fit_transform не только преобразует тексты в вектора, но и преобразует в словарь\n",
    "#transform из ячеек ниже просто преобразует"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier().fit(X_train_counts, twenty_train.target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, \n",
    "                                 shuffle=True, random_state=0)\n",
    "X_test = count_vect.transform(twenty_test.data) # Extract token counts out of raw text documents using the vocabulary fitted with fit \n",
    "print(X_test.shape)\n",
    "predicted = clf.predict(X_test)\n",
    "predicted_proba = clf.predict_proba(X_test) #если хотим вероятности"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = accuracy_score(predicted, twenty_test.target) #twenty_test.target – реальная разметка текста\n",
    "micro_f1 = f1_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_p = precision_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_r = recall_score(predicted, twenty_test.target, average = 'micro')\n",
    "macro_f1 = f1_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_p = precision_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_r = recall_score(predicted, twenty_test.target, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(twenty_test.target, predicted), annot=True, fmt=\"d\", cbar=False, xticklabels=categories, yticklabels=categories)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{} => {}'.format(doc, twenty_train.target_names[category]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Метод наивного Байеса  [Multinomial naive Bayes classifier]\n",
    "\n",
    "Требуется оценить вероятность принадлежности документа $d \\in D$ классу $c \\in C$: $p(c|d)$. Каждый документ –  мешок слов, всего слов $|V|$.\n",
    "\t\n",
    "$p(c)$ – априорная вероятность класса $c$\n",
    "   \n",
    "$p(c|d)$ – апостериорная вероятность класса $c$\n",
    "\t\n",
    "\t\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$ p(c|d) = \\frac{p(d|c)p(c)}{p(d)} $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Пусть документ $d$ описан признаками $f_1, \\dots, f_N$.\n",
    "\n",
    "$ c_{NB} = \\arg \\max _{c \\in C} p (c|d) = \\arg \\max_{c \\in C}  \\frac{p(d|c)p(c)}{p(d)} \\propto $\n",
    "\t\n",
    "$ \\propto \\arg \\max_{c \\in C} p(d|c)p(c)  = \\arg \\max_{c \\in C} p(f_1, f_2, \\dots, f_{N} | c)p(c)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Предположение о независимости \n",
    "\n",
    "* Мешок слов: порядок слов не имеет значения\n",
    "* Условная независимость (наивное предположение): вероятности признаков $p(f_i|c_j)$ внутри класса $c_j$ независимы\n",
    "\n",
    "$p(f_1, f_2, \\dots, f_{N} | c) \\times  p(c) =   p(f_1|c) \\times p(f_2|c) \\times \\dots \\times p(f_{N}|c)  \\times p(c)$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$C_{NB}=\\arg \\max_{c \\in C} p(c) \\times \\prod_{1 \\le i \\le N} p(f_i|c) $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Допустим, что признаки $f_i$ – слова $w_i$, а $\\texttt{positions}$ – все позиции слов в документе.\n",
    "\n",
    "\n",
    "$C_{NB} = p(c) \\times \\prod_{i \\in \\texttt{positions}} p(w_i|c) $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/echernyak/ML-for-compling/d6b4f82e788cd7b365ea711db2ac4b0fc7a361d0/img/bow.png\" width=\"600\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обучение наивного Байесовского классификатора\n",
    "\n",
    "#### ММП оценки вероятностей:\n",
    "\t\n",
    "$ \\widehat{p_(c_j)} = \\frac{| \\{d| d \\in c_j\\} |}{|D|} $\n",
    "\t\n",
    "$ \\widehat{p(w_i | c_j)} = \\frac{\\texttt{count}(w_i, c_j)}{\\sum_{w \\in V} \\texttt{count}(w, c_j)} $\n",
    "\t\n",
    "Создаем $|C|$ мегадокументов: каждый документ = все документы в одном классе, склеенные в один мегадокумент и вычисляем частоты $w$ в мегадокументах.\n",
    "\t\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Проблема нулевых вероятностей:  \n",
    "\n",
    "$\\texttt{count}(w_i, c_j)$ может быть равно нулю. \n",
    "\n",
    "Допустим, что каждое слово встречается как минимум $\\alpha$ раз в мешке слов.\n",
    "\t\n",
    "Преобразование Лапласа: $ \\frac{+\\alpha}{+\\alpha |V|}$\n",
    "\t\n",
    "$ \\widehat{p(w_i | c_j)} = \\frac{\\texttt{count}(w_i, c_j) + \\alpha}{(\\sum_{w \\in V} \\texttt{count}(w, c_j)) + \\alpha |V| } $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Пример. Тематическая классификация\n",
    "\t\n",
    "    \n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>документ</th>\n",
    "    <th>класс</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\">обучающее<br>множество</td>\n",
    "    <td>Chinese Beijing Chinese</td>\n",
    "    <td>c</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Chinese Chinese Shanghai</td>\n",
    "    <td>c</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Chinese Macao</td>\n",
    "    <td>c</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Tokyo Japan Chinese</td>\n",
    "    <td>j</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>тестовое<br>множество</td>\n",
    "    <td>Chinese Chinese Chinese Tokyo Japan</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$p(c) =\\frac{3}{4}$,$p(j) = \\frac{1}{4}$\n",
    "\n",
    "$p(\\texttt{Chinese|c)}= (5+1)/(8+6)=6/14=3/7$  \n",
    "\n",
    "$p(\\texttt{Chinese|j)}= (1+1)/(3+6)=2/9$  \n",
    "\n",
    "$p(\\texttt{Tokyo|c)}= (0+1)/(8+6)=1/14$  \n",
    "\n",
    "$p(\\texttt{Tokyo|j)}= (1+1)/(3+6)=2/9$  \n",
    "\n",
    "$p(\\texttt{Japan|c)}= (0+1)/(8+6)=1/14$  \n",
    "\n",
    "$p(\\texttt{Japan|j)}= (1+1)/(3+6)=2/9$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$p(c|d_5) = 3/4 \\times (3/7)^3 \\times 1/14 \\times 1/14 \\approx 0.0003$\n",
    "\n",
    "$p(j|d_5) = 1/4 \\times (2/9)^3 \\times 2/9 \\times 2/9 \\approx 0.0001$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Мультиномиальный наивный Байсовский классификатор\n",
    "\n",
    "Слова в тексте распределены по мультиномиальному закону: документ – это последовательность событий. Каждое событие – это случайный выбор одного слова из мешка слов. Мы перемножаем вероятности встреченных слов в предположении, что каждое событие независимо. Таким образом мы получаем мультиномиальную  модель. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, twenty_train.target)\n",
    "predicted = clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = accuracy_score(predicted, twenty_test.target)\n",
    "micro_f1 = f1_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_p = precision_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_r = recall_score(predicted, twenty_test.target, average = 'micro')\n",
    "macro_f1 = f1_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_p = precision_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_r = recall_score(predicted, twenty_test.target, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(data=confusion_matrix(twenty_test.target, predicted), annot=True, fmt=\"d\", cbar=False, xticklabels=categories, yticklabels=categories)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{} => {}'.format(doc, twenty_train.target_names[category]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Логистическая регрессия (метод максимальной энтропии [MaxEnt])\n",
    "\n",
    "Требуется оценить вероятность принадлежности документа $d \\in D$ классу $c \\in C$: $p(c|d)$. Пусть заданы признаки  $f_i \\in F$ – множество признаков и  $w_i$ – их веса. \n",
    "\n",
    "Признаки могут зависеть от классов: $f_i(c,d)$  \n",
    "\n",
    "Линейная комбинация этих признаков: $\\sum_{i=1}^k w_i f_i(c,d)$.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как связана $\\sum_{i=1}^k w_i f_i(c,x)$ и $p(c|d)$?\n",
    "\t\n",
    "$p(c|d) = \\frac{1}{Z} e^{\\sum_{i=1}^k w_i f_i(c,d)},$\n",
    "\t\n",
    "где $\\frac{1}{Z} = \\frac{1}{\\sum_{c' \\in C} e^{\\sum_{i=1}^k w_i f_i(c',d)} }.$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$ \\widehat{c} = \\texttt{argmax}_{c \\in C} p (c|d) = \\texttt{argmax}_{c \\in C} \\frac{e^{\\sum_{i=1}^k w_i f_i(c,d)}}{\\sum_{c' \\in C} e^{\\sum_{i=1}^k w_i f_i(c',d)}}  \\propto  \\texttt{argmax}_{c \\in C} e^{\\sum_{i=1}^k w_i f_i(c,d)}   \\propto  \\texttt{argmax}_{c \\in C} \\sum_{i=1}^k w_i f_i(c,d).  $\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Пример. Классификация по тональности на $C = <+,->$\n",
    "\t\n",
    "Используем индикаторные признаки\n",
    "\t\n",
    "\n",
    "*... there are virtually no surprises, and the writing is second-rate. So why did I enjoy it so much? For one thing, the cast is great ...*\n",
    "\t \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>признак</th>\n",
    "    <th>значение</th>\n",
    "    <th></th>\n",
    "    <th>вес</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$f_1$</td>\n",
    "    <td>1</td>\n",
    "    <td> \"great\" $\\in d$ и $c=+$</td>\n",
    "    <td rowspan=\"2\">1.9</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>иначе</td>\n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>$f_2$</td>\n",
    "    <td>1</td>\n",
    "    <td>\"second-rate\" $\\in d$ и $c=-$</td>\n",
    "    <td rowspan=\"2\">0.9</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>иначе</td>\n",
    "  </tr>\n",
    "\n",
    "   <tr>\n",
    "    <td>$f_3$</td>\n",
    "    <td>1</td>\n",
    "    <td>\"no\" $\\in d$ и $c=-$ </td>\n",
    "    <td rowspan=\"2\">0.7</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>иначе</td>\n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>$f_4$</td>\n",
    "    <td>1</td>\n",
    "    <td>\"enjoy\" $\\in d$ и $c=-$ </td>\n",
    "    <td rowspan=\"2\">-0.8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>иначе</td>\n",
    "  </tr>\n",
    "\n",
    "   <tr>\n",
    "    <td>$f_4$</td>\n",
    "    <td>1</td>\n",
    "    <td>\"great\" $\\in d$ и $c=-$ </td>\n",
    "    <td rowspan=\"2\">-0.6</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>иначе</td>\n",
    "  </tr>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</table>\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "класс $+$:  $1.9 + 0 + 0 + 0 + 0 = 1.9$\n",
    "\t\n",
    "класс $-$: $0 + 0.9 + 0.7 - 0.8 - 0.6 =0.2$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$p(+|d) = \\frac{e^{1.9}}{e^{1.9}+e^{0.2}}$\n",
    "\t\n",
    "$p(-|d) = \\frac{e^{0.2}}{e^{1.9}+e^{0.2}}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(X_train_counts, twenty_train.target)\n",
    "predicted = clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = accuracy_score(predicted, twenty_test.target)\n",
    "micro_f1 = f1_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_p = precision_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_r = recall_score(predicted, twenty_test.target, average = 'micro')\n",
    "macro_f1 = f1_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_p = precision_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_r = recall_score(predicted, twenty_test.target, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(data=confusion_matrix(twenty_test.target, predicted), annot=True, fmt=\"d\", cbar=False, xticklabels=categories, yticklabels=categories)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{} => {}'.format(doc, twenty_train.target_names[category]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Метод опорных векторов [Support vector machine, SVM]\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/echernyak/ML-for-compling/d6b4f82e788cd7b365ea711db2ac4b0fc7a361d0/img/svm_1.png\" width=\"200\" align='right'>\n",
    "\n",
    "\n",
    "$a(x) = sign(<w,x>+b)$ – классификатор, задающий разделяющую гиперплоскость\n",
    "\n",
    "$ \\min_{x \\in X} |<w,x>+b| = 1$ – нормировка параметров\n",
    "\n",
    "Требуется построить разделяющую гиперплоскость шириной $ \\frac{2}{||w||}$ (т.е. $2 ~ \\times $ расстояние от разделяющей гиперплоскости до ближайшего объекта обучающего множества, иначе отступ).\n",
    "\n",
    "\n",
    "Оптимизационная задача (если выборка линейно разделима):\n",
    "\n",
    "$ \\frac{1}{2} ||w||^2 \\rightarrow \\min_{w,b} $\n",
    "\n",
    "$ y_i(<w_i, x_i> + b) \\geq 1 $\n",
    "\n",
    "\n",
    "\n",
    "Конспекты Е. Соколова: https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture05-linclass.pdf\n",
    "\n",
    "Multi-class SVM: https://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC().fit(X_train_counts, twenty_train.target)\n",
    "predicted = clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = accuracy_score(predicted, twenty_test.target)\n",
    "micro_f1 = f1_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_p = precision_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_r = recall_score(predicted, twenty_test.target, average = 'micro')\n",
    "macro_f1 = f1_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_p = precision_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_r = recall_score(predicted, twenty_test.target, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(data=confusion_matrix(twenty_test.target, predicted), annot=True, fmt=\"d\", cbar=False, xticklabels=categories, yticklabels=categories)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{} => {}'.format(doc, twenty_train.target_names[category]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Деревья решений \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/echernyak/ML-for-compling/d6b4f82e788cd7b365ea711db2ac4b0fc7a361d0/img/decision-tree.png\" width=\"300\" align='center'>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier().fit(X_train_counts, twenty_train.target)\n",
    "predicted = clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = accuracy_score(predicted, twenty_test.target)\n",
    "micro_f1 = f1_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_p = precision_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_r = recall_score(predicted, twenty_test.target, average = 'micro')\n",
    "macro_f1 = f1_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_p = precision_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_r = recall_score(predicted, twenty_test.target, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(data=confusion_matrix(twenty_test.target, predicted), annot=True, fmt=\"d\", cbar=False, xticklabels=categories, yticklabels=categories)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Отбор признаков\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Веса** :\n",
    "* $tf-idf$, $\\chi^2$ для взвешивания слов\n",
    "* меры ассоциации биграм для отбора биграм: $(P)PMI$, $t-score$\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Уменьшение количества признаков**:\n",
    "* лемматизация\n",
    "* стемминг\n",
    "* удаление стоп-слов\n",
    "* пороги на частоту ($min\\_tf$)\n",
    "* пороги на документную частоту ($max\\_df$)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Признаки**:\n",
    "* $n$-грамы \n",
    "* символьные $n$-грамы  (подслова, subwords)\n",
    "* именованные сущности \n",
    "* термины\n",
    "* \"не\\_\" + слово \n",
    "* сохраним $N$ самых частых слов, остальные представим подсловами и символьными\n",
    "$n$-грамами\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "** Снижение размерности **:\n",
    "* скрытые темы в качестве признаков"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Конвеер в sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range = (2,3))), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "text_clf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "predicted = text_clf.predict(twenty_test.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = accuracy_score(predicted, twenty_test.target)\n",
    "micro_f1 = f1_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_p = precision_score(predicted, twenty_test.target, average = 'micro')\n",
    "micro_r = recall_score(predicted, twenty_test.target, average = 'micro')\n",
    "macro_f1 = f1_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_p = precision_score(predicted, twenty_test.target, average = 'macro')\n",
    "macro_r = recall_score(predicted, twenty_test.target, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{} => {}'.format(doc, twenty_train.target_names[category]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}